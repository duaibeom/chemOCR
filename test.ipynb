{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model.utils.transfroms import get_train_transform, get_valid_transform, get_test_transform\n",
    "from model.dataset import CustomDataset\n",
    "\n",
    "HOME = ''\n",
    "HOME_DIR = f'{HOME}/Data/CHEMBL/OCR'\n",
    "DATAFRAME_LIST = dict(train=f'{HOME}/Data/chembl_30_chemreps_train.csv',\n",
    "                        val=f'{HOME}/Data/chembl_30_chemreps_eval.csv',\n",
    "                        test=f'{HOME}/Data/chembl_30_chemreps_test.csv')\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "ocr_dataset = CustomDataset(\n",
    "        data_df=DATAFRAME_LIST['val'],\n",
    "        # mode='val',\n",
    "        mode='test',\n",
    "        # transforms=get_valid_transform(),\n",
    "        # transforms=get_train_transform(),\n",
    "        transforms=get_test_transform(),\n",
    "        dir_path=HOME_DIR,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, gt_shr, gt_shr_mask, gt_thr, gt_thr_mask = ocr_dataset[14]\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(24, 6))\n",
    "ax[0].imshow(image[0])\n",
    "ax[1].imshow(gt_shr)\n",
    "ax[2].imshow(gt_shr_mask)\n",
    "ax[3].imshow(gt_thr)\n",
    "ax[4].imshow(gt_thr_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from model.dbnet import DBNet\n",
    "from model.loss import DBLoss\n",
    "\n",
    "model = DBNet(\n",
    "        inner_channels=128,\n",
    "        out_channels=64,\n",
    "        head_in_channels=320,\n",
    "        test=True,\n",
    "    )\n",
    "\n",
    "model.load_state_dict(torch.load('model_weights.MobileNetV3Small.pth'), strict=False)\n",
    "# model.cpu()\n",
    "model.eval()\n",
    "\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image, gt_shr, gt_shr_mask, gt_thr, gt_thr_mask = ocr_dataset[idx]\n",
    "image = ocr_dataset[idx]\n",
    "# image = ocr_dataset[54]\n",
    "image = image[None,]\n",
    "\n",
    "x = model(image)\n",
    "print(idx)\n",
    "\n",
    "fig, ax = plt.subplots(2, 4, figsize=(24, 10))\n",
    "ax[0, 0].imshow(image[0, 0].detach().numpy())\n",
    "# ax[0, 1].imshow(x[0, 0].detach().numpy())\n",
    "# ax[0, 2].imshow(x[0, 1].detach().numpy())\n",
    "# ax[0, 3].imshow(x[0, 2].detach().numpy())\n",
    "ax[0, 1].imshow(x[0, 1].detach().numpy())\n",
    "ax[0, 2].imshow(x[0, 2].detach().numpy())\n",
    "ax[0, 3].imshow(x[0, 3].detach().numpy())\n",
    "ax[1, 0].imshow(x[0, 4].detach().numpy())\n",
    "ax[1, 1].imshow(x[0, 5].detach().numpy())\n",
    "ax[1, 2].imshow(x[0, 6].detach().numpy())\n",
    "ax[1, 3].imshow(x[0, 7].detach().numpy())\n",
    "# ax[1, 0].imshow(gt_shr.detach().numpy())\n",
    "# ax[1, 1].imshow(gt_shr.detach().numpy() == 5)\n",
    "# ax[1, 2].imshow(gt_shr.detach().numpy() == 6)\n",
    "# ax[1, 3].imshow(gt_shr.detach().numpy() == 7)\n",
    "\n",
    "idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image, gt_shr, gt_shr_mask, gt_thr, gt_thr_mask = ocr_dataset[idx]\n",
    "image = ocr_dataset[idx]\n",
    "# image = Tensor(image[None, None, :, :])\n",
    "image = image[None,]\n",
    "\n",
    "x = model(image)\n",
    "\n",
    "fig, ax = plt.subplots(2, 4, figsize=(24, 10))\n",
    "ax[0, 0].imshow(image[0, 0].detach().numpy())\n",
    "# ax[0, 1].imshow(x[0, 0].detach().numpy())\n",
    "# ax[0, 2].imshow(x[0, 3].detach().numpy())\n",
    "ax[0, 1].imshow(x[0, 4].detach().numpy())\n",
    "ax[0, 2].imshow(x[0, 5].detach().numpy())\n",
    "ax[0, 3].imshow(x[0, 6].detach().numpy())\n",
    "ax[1, 0].imshow(x[0, 7].detach().numpy())\n",
    "ax[1, 1].imshow(x[0, 8].detach().numpy())\n",
    "ax[1, 2].imshow(x[0, 9].detach().numpy())\n",
    "ax[1, 3].imshow(x[0, 10].detach().numpy())\n",
    "\n",
    "idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, 3, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, 2, 2),\n",
    "            nn.Conv2d(256, 256, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        # self.fc = nn.Linear(256, 62)\n",
    "        self.fc = nn.Conv2d(256, 512, 1)\n",
    "        self.fc1 = nn.Conv2d(512, 62, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        # x = x.view(256, -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.fc1(x)\n",
    "        x = x.squeeze(-1)\n",
    "        x = x.squeeze(-1)\n",
    "        return x\n",
    "\n",
    "char_model = NeuralNetwork()\n",
    "char_model.load_state_dict(torch.load(\"model_weights.emnist.pth\"))\n",
    "\n",
    "char_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eminst_class = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "convT2d = torch.nn.ConvTranspose2d(320, 320, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from shapely.geometry import Polygon\n",
    "import numpy as np\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "\n",
    "def sqrt_einsum(a, b):\n",
    "    a_min_b = a - b\n",
    "    return np.sqrt(np.einsum(\"i,i->\", a_min_b, a_min_b))\n",
    "\n",
    "def cal_avg_dist(contours):\n",
    "    _length = []\n",
    "    limit = 0\n",
    "    for _polygon in contours:\n",
    "        limit += 1\n",
    "        if limit == 30:\n",
    "            break\n",
    "        _length.append(sqrt_einsum(_polygon.max(axis=0)[0], _polygon.min(axis=0)[0]))\n",
    "    _length = np.array(_length, dtype=np.float16)\n",
    "    return _length.mean()\n",
    "\n",
    "# def chk_pair(dist_arr):\n",
    "#     _, _dist_arr = np.where(dist_arr == 1)\n",
    "#     if np.all(dist_arr.sum(axis=1) == 2):\n",
    "#         return _dist_arr.reshape(-1, 2).astype(int)\n",
    "#     else:\n",
    "#         plt.imshow(dist_arr)\n",
    "#         raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = ocr_dataset[54]\n",
    "image, gt_shr, gt_shr_mask, gt_thr, gt_thr_mask = ocr_dataset[2]\n",
    "image = image[None,]\n",
    "\n",
    "neck_out = model.neck(image)\n",
    "out = model.head(neck_out)\n",
    "out = out.detach().cpu().numpy()\n",
    "\n",
    "x = model.head.categorize[:-2](neck_out)\n",
    "x = convT2d(x)\n",
    "x = x.detach().numpy()\n",
    "\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair(array1, array2):\n",
    "    _dist = np.abs(array1[None, :, :] - array2[:, None, :]).max(axis=2)\n",
    "    return _dist.min(axis=0).argsort()[:2]\n",
    "\n",
    "def _buffered_polygon_coords(polygon):\n",
    "    if not isinstance(polygon, Polygon):\n",
    "        polygon = Polygon(polygon)\n",
    "    area = polygon.area\n",
    "    peri = polygon.length\n",
    "    buffer_polygon = polygon.buffer(distance=2, cap_style=2, join_style=2)\n",
    "    return np.array(buffer_polygon.exterior.coords)\n",
    "\n",
    "def extreme_points(contour):\n",
    "    extreme_left = tuple(contour[contour[:, :, 0].argmin()][0])\n",
    "    extreme_right = tuple(contour[contour[:, :, 0].argmax()][0])\n",
    "    extreme_top = tuple(contour[contour[:, :, 1].argmin()][0])\n",
    "    extreme_bottom = tuple(contour[contour[:, :, 1].argmax()][0])\n",
    "    return extreme_left, extreme_right, extreme_top, extreme_bottom\n",
    "\n",
    "def get_min_max(contours):\n",
    "    _list = []\n",
    "    for _polygon in contours:\n",
    "        max_x, max_y = _polygon.max(axis=0)[0]\n",
    "        min_x, min_y = _polygon.min(axis=0)[0]\n",
    "        _max = [ceil(max_x)+3, ceil(max_y)+3]\n",
    "        _min = [int(min_x)-2, int(min_y)-2]\n",
    "        _list.append([_max, _min])\n",
    "    return _list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "contours = {}\n",
    "for idx in range(1, 3):\n",
    "    #TODO: var: img\n",
    "    _contours, _ = cv2.findContours(255*np.array(out[0][idx] > 0.2, dtype=np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    _ctrs = []\n",
    "    if _contours.__len__() == 0:\n",
    "        contours[idx] = None\n",
    "        continue\n",
    "    for _polygon in _contours:\n",
    "        if _polygon.shape.__len__() > 1:\n",
    "            rect = cv2.minAreaRect(_polygon)\n",
    "            (x, y), (w, h), ang = rect\n",
    "            box = cv2.boxPoints(rect)\n",
    "            _ctrs.append([x, y])\n",
    "    contours[idx] = np.array(_ctrs, dtype=np.float16)\n",
    "    if idx == 2:\n",
    "        char_pos = get_min_max(_contours)\n",
    "\n",
    "def chunk_char(array, cutoff):\n",
    "    _dist_arr = np.abs(array[None, :, :] - array[:, None, :]).max(axis=2)\n",
    "    _dist_arr[np.triu_indices(_dist_arr.shape[0])] = 999\n",
    "    _dist = np.where(_dist_arr < cutoff * 1.2)\n",
    "    _dist = np.stack(_dist, axis=1)\n",
    "    return _dist\n",
    "\n",
    "b_pair = {}\n",
    "polygons = {}\n",
    "avg_b_dist = 0\n",
    "bond_length = []\n",
    "for idx in range(3, 8):\n",
    "    #TODO: var: img\n",
    "    _contours, _ = cv2.findContours(255*np.array(out[0][idx] > 0.2, dtype=np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    _ctrs = []\n",
    "    _poly = []\n",
    "    if _contours.__len__() == 0:\n",
    "        contours[idx] = None\n",
    "        polygons[idx] = None\n",
    "        continue\n",
    "    for _polygon in _contours:\n",
    "        # _polygon = _polygon.squeeze()\n",
    "        if cv2.contourArea(_polygon) > 1:\n",
    "            rect = cv2.minAreaRect(_polygon)\n",
    "            (x, y), (w, h), ang = rect\n",
    "            box = cv2.boxPoints(rect)\n",
    "            _ctrs.append([x, y])\n",
    "            _poly.append(box)\n",
    "            bond_length.append([w, h])\n",
    "\n",
    "    contours[idx] = np.array(_ctrs, dtype=np.float16)\n",
    "    polygons[idx] = np.array(_poly, dtype=np.float16)\n",
    "\n",
    "bond_avg_length = np.array(bond_length, dtype=np.float16).max(axis=1).mean()\n",
    "chunked_char = chunk_char(contours[2], bond_avg_length)\n",
    "\n",
    "heavy_atom = np.ones(contours[2].__len__(), dtype=np.uint8)\n",
    "\n",
    "pred_char_list = []\n",
    "\n",
    "for i in char_pos:\n",
    "    _max, _min = i\n",
    "    _image = image[:, :, _min[1]:_max[1], _min[0]:_max[0]]\n",
    "    # idx += 1\n",
    "    pred = eminst_class[char_model(_image).argmax()]\n",
    "    if pred in ['0', 'Q']:\n",
    "        pred = 'O'\n",
    "    \n",
    "    pred_char_list.append(pred)\n",
    "\n",
    "for i in chunked_char:\n",
    "    a = pred_char_list[i[0]]\n",
    "    b = pred_char_list[i[1]]\n",
    "    if a == 'H':\n",
    "        heavy_atom[i[0]] = 0\n",
    "    elif b == 'H':\n",
    "        heavy_atom[i[1]] = 0\n",
    "\n",
    "contours[2] = contours[2][np.where(heavy_atom == 1)]\n",
    "pred_char_list = np.array(pred_char_list)[np.where(heavy_atom == 1)]\n",
    "\n",
    "pts = np.concatenate([contours[1], contours[2]], axis=0)\n",
    "\n",
    "for idx in range(3, 8):\n",
    "    if polygons[idx] is not None:\n",
    "        b_pair[idx] = []\n",
    "        for _polygon in polygons[idx]:\n",
    "            b_pair[idx].append(get_pair(pts, _polygon))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "from rdkit import Chem, rdBase\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import rdDistGeom, rdDepictor, rdCoordGen\n",
    "\n",
    "IPythonConsole.drawOptions.addAtomIndices = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_type_list = {\n",
    "    3: Chem.BondType.SINGLE,\n",
    "    4: Chem.BondType.DOUBLE,\n",
    "    5: Chem.BondType.TRIPLE,\n",
    "    6: Chem.BondType.SINGLE,\n",
    "    7: Chem.BondType.SINGLE,\n",
    "}\n",
    "b_dir_list = {\n",
    "    6: Chem.BondDir.BEGINWEDGE,\n",
    "    7: Chem.BondDir.BEGINDASH,\n",
    "}\n",
    "\n",
    "a_list = {\n",
    "    'H': 1,\n",
    "    'C': 6,\n",
    "    'N': 7,\n",
    "    'O': 8,\n",
    "    'F': 9,\n",
    "    'E': 9,\n",
    "    'R': 9,\n",
    "    'S': 16,\n",
    "}\n",
    "\n",
    "mol = Chem.RWMol()\n",
    "\n",
    "a_idx = 0\n",
    "for _ in contours[1]:\n",
    "    atom = Chem.Atom(6)\n",
    "    mol.AddAtom(atom)\n",
    "    a_idx += 0\n",
    "\n",
    "for _ in pred_char_list:\n",
    "    atom = Chem.Atom(a_list[_])\n",
    "    mol.AddAtom(atom)\n",
    "    a_idx += 0\n",
    "\n",
    "b_idx = 0\n",
    "chiral_b_idx = {}\n",
    "for idx in b_pair:\n",
    "    for i, j in b_pair[idx]:\n",
    "        i = int(i)\n",
    "        j = int(j)\n",
    "        mol.AddBond(i, j, order=b_type_list[idx])\n",
    "        if idx > 5:\n",
    "            _bond = mol.GetBondWithIdx(b_idx)\n",
    "            _bond.SetBondDir(b_dir_list[idx])\n",
    "            chiral_b_idx[b_idx] = [i, j]\n",
    "        b_idx += 1\n",
    "\n",
    "\n",
    "conf = Chem.Conformer(mol.GetNumAtoms())\n",
    "mol.AddConformer(conf)\n",
    "\n",
    "Chem.SanitizeMol(mol)\n",
    "# rdDepictor.Compute2DCoords(mol)\n",
    "Chem.AssignChiralTypesFromBondDirs(mol)\n",
    "\n",
    "if chiral_b_idx.__len__() != 0:\n",
    "    for _idx in chiral_b_idx:\n",
    "        _b = mol.GetBondWithIdx(_idx)\n",
    "        _bDir = _b.GetBondDir()\n",
    "        _aI = _b.GetBeginAtom()\n",
    "        _aJ = _b.GetEndAtom()\n",
    "        type_bool1 = _aI.GetChiralTag() == Chem.ChiralType.CHI_UNSPECIFIED\n",
    "        type_bool2 = _aJ.GetChiralTag() == Chem.ChiralType.CHI_UNSPECIFIED\n",
    "        if type_bool1 and type_bool2:\n",
    "            j, i = chiral_b_idx[_idx]\n",
    "            mol.RemoveBond(j, i)\n",
    "            mol.AddBond(i, j, order=Chem.BondType.SINGLE)\n",
    "            _b = mol.GetBondWithIdx(b_idx-1)\n",
    "            _b.SetBondDir(_bDir)\n",
    "\n",
    "Chem.AssignChiralTypesFromBondDirs(mol)\n",
    "\n",
    "# Chem.DetectBondStereochemistry(mol)\n",
    "# Chem.AssignAtomChiralTagsFromStructure(mol)\n",
    "# Chem.AssignStereochemistry(mol)\n",
    "\n",
    "Chem.SanitizeMol(mol)\n",
    "smi = Chem.MolToSmiles(mol)\n",
    "mol = Chem.MolFromSmiles(smi)\n",
    "# mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgparams = rdCoordGen.CoordGenParams()\n",
    "cgparams.minimizerPrecision = cgparams.sketcherBestPrecision\n",
    "rdCoordGen.AddCoords(mol, cgparams)\n",
    "mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(b_pair[3][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(24, 10))\n",
    "ax[0, 0].imshow(image[0, 0].detach().numpy())\n",
    "ax[0, 1].imshow(out[0, 1])\n",
    "ax[0, 2].imshow(out[0, 2])\n",
    "ax[0, 3].imshow(out[0, 3])\n",
    "ax[1, 0].imshow(out[0, 4])\n",
    "ax[1, 1].imshow(out[0, 5])\n",
    "ax[1, 2].imshow(out[0, 6])\n",
    "ax[1, 3].imshow(out[0, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in contours[1]: plt.scatter(i[0], i[1], s=4200, c='none', edgecolors='k', alpha=0.2)\n",
    "k = 0\n",
    "for i in contours[1]: plt.scatter(i[0], i[1], c='k', alpha=0.5)\n",
    "for i in contours[1]: plt.text(i[0], i[1], k); k += 1\n",
    "k = 0\n",
    "# for i in contours[2]: plt.scatter(i[0], i[1], s=4200, c='none', edgecolors='b', alpha=0.2)\n",
    "for i in contours[2]: plt.scatter(i[0], i[1], c='b', alpha=0.5)\n",
    "for i in contours[2]: plt.text(i[0], i[1], k); k += 1\n",
    "k = 0\n",
    "for i in contours[6]: plt.scatter(i[0], i[1], c='gray', alpha=0.5)\n",
    "for i in contours[6]: plt.text(i[0], i[1], k, c='gray'); k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "# from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.cuda.amp.grad_scaler import GradScaler\n",
    "from torch.cuda.amp.autocast_mode import autocast\n",
    "\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose([\n",
    "        A.Resize(14, 14),\n",
    "        A.HorizontalFlip(True),\n",
    "        A.Rotate((90, 90), always_apply=True),\n",
    "        # A.Blur(blur_limit=4, p=0.3),\n",
    "        # A.RandomBrightnessContrast(p=0.3),\n",
    "        A.GaussNoise(var_limit=(0, 0.2), p=0.3),\n",
    "        # A.CoarseDropout(4, 3, p=0.3),\n",
    "        A.InvertImg(p=1),\n",
    "        A.Normalize(mean=(0.45), std=(0.22), max_pixel_value=255),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "test_transform = A.Compose([\n",
    "        A.Resize(14, 14),\n",
    "        A.HorizontalFlip(True),\n",
    "        A.Rotate((90, 90), always_apply=True),\n",
    "        A.InvertImg(p=1),\n",
    "        A.Normalize(mean=(0.45), std=(0.22), max_pixel_value=255),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "def get_train_transform(image):\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    x = train_transform(image=image)\n",
    "    return x['image']\n",
    "\n",
    "def get_test_transform(image):\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    x = test_transform(image=image)\n",
    "    return x['image']\n",
    "\n",
    "\n",
    "training_data = datasets.EMNIST(\n",
    "    split=\"byclass\", \n",
    "    # split='letters',\n",
    "    root=\"data\", \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=get_train_transform\n",
    ")\n",
    "\n",
    "test_data = datasets.EMNIST(\n",
    "    split=\"byclass\",\n",
    "    # split='letters',\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=get_test_transform,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    training_data, batch_size=1024, shuffle=True, num_workers=4\n",
    ")\n",
    "test_dataloader = DataLoader(test_data, batch_size=1024, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, 3, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, 2, 2),\n",
    "            nn.Conv2d(256, 256, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        )\n",
    "        # self.fc = nn.Linear(256, 62)\n",
    "        self.fc = nn.Conv2d(256, 512, 1)\n",
    "        self.fc1 = nn.Conv2d(512, 62, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        # x = x.view(256, -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.fc1(x)\n",
    "        x = x.squeeze(-1)\n",
    "        x = x.squeeze(-1)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, autocast_enabled):\n",
    "\n",
    "    if autocast_enabled:\n",
    "        scaler = GradScaler()\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True), autocast(enabled=autocast_enabled):\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            if not autocast_enabled:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(training_data[idx][0][0])\n",
    "print(training_data.classes[training_data[idx][1]])\n",
    "idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1, 1, 28, 28)).to(device)\n",
    "\n",
    "for name, module in model.conv_stack.named_children():\n",
    "    in_shape = x.shape\n",
    "    x = model.conv_stack[int(name)](x)\n",
    "    print(name, in_shape, x.shape)\n",
    "\n",
    "in_shape = x.shape\n",
    "x = model.fc(x)\n",
    "print(name, in_shape, x.shape)\n",
    "in_shape = x.shape\n",
    "x = x.squeeze(-1)\n",
    "x = x.squeeze(-1)\n",
    "print(name, in_shape, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer, True)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "    # if t % 5 == 0:\n",
    "print(\"Done!\")\n",
    "\n",
    "torch.save(model.state_dict(), \"model_weights.emnist.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('rdkit310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "245b9d6615daa1a3827d07c8eb27e2623b2ed10fac89f2a0a36a803424ad4263"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
